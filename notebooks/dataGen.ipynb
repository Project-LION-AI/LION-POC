{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-sYrq3PaGCq"
      },
      "source": [
        "TODO:\n",
        "\n",
        "\n",
        "\n",
        "*  Modularize data generation\n",
        "    * Structure data-gen class\n",
        "*  Expand dataset selection\n",
        "    * Enron dataset\n",
        "    * Discord scraper\n",
        "    * Stanford paper (add link to this)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbRJQXNcABZ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "XKTbQ_YxTjvP",
        "outputId": "0dda481e-019b-4643-a171-3c6613e56bec"
      },
      "outputs": [],
      "source": [
        "# INSTALL GENERAL DEPENDENCIES\n",
        "\n",
        "#! python3\n",
        "!pip install openai\n",
        "# Downgrade pandas to be compatible with Colab\n",
        "!pip install pandas==1.1.0\n",
        "\n",
        "#libraries\n",
        "import os\n",
        "import openai\n",
        "\n",
        "import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHxCo4CadiJQ",
        "outputId": "3abc985c-6da8-4458-e39c-0129cb49ba2f"
      },
      "outputs": [],
      "source": [
        "# Connect environment variables. If there is not a vars.env file in your drive, colab-env will create one. You can directly add variables\n",
        "# to the file in a text editor, or use the following sample code to add. (Remember to delete the calls before committing, or it defeats the purpose!)\n",
        "!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "#colab_env.envvar_handler.add_env('var','value', overwrite=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp-RcVyxBkHJ",
        "outputId": "520b936b-ad00-4635-e1ad-afa06ea8aec5"
      },
      "outputs": [],
      "source": [
        "!more gdrive/My\\ Drive/vars.env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfMulLHYagFQ",
        "outputId": "5481c9b0-a215-40bf-fdab-376442dc7cb8"
      },
      "outputs": [],
      "source": [
        "# CONNECT KAGGLE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/\n",
        "!cat ~/.kaggle/kaggle.json \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Format of kaggle download command (non-competition datasets). ENRON EMAILS DATASET CRASHES JESSIE'S LAPTOP\n",
        "#!kaggle datasets download -d wcukierski/enron-email-dataset -p enron_emails\n",
        "\n",
        "# SAMPLE CODE; DESIRED DATASET TOO BIG\n",
        "\n",
        "# Alternate way to download kaggle files\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "# Download all\n",
        "api.dataset_download_files('imdevskp/corona-virus-report')\n",
        "\n",
        "# Download 1\n",
        "api.dataset_download_file('imdevskp/corona-virus-report','covid_19_clean_complete.csv')\n",
        "\n",
        "zf = ZipFile('covid_19_clean_complete.csv.zip')\n",
        "#extracted data is saved in the same directory as notebook\n",
        "zf.extractall() \n",
        "zf.close()\n",
        "\n",
        "import pandas as pd\n",
        "data=pd.read_csv('covid_19_clean_complete.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cluIuLnyUP1C",
        "outputId": "3eb2ea75-07c1-40e8-d5b2-df1d513b5d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting tweets before 1476076237432901632\n",
            "...400 tweets downloaded so far\n",
            "getting tweets before 1472332160543559681\n",
            "...600 tweets downloaded so far\n",
            "getting tweets before 1468350290092908544\n",
            "...800 tweets downloaded so far\n",
            "getting tweets before 1461187944123748357\n",
            "...1000 tweets downloaded so far\n",
            "getting tweets before 1456288787831537665\n",
            "...1200 tweets downloaded so far\n",
            "getting tweets before 1453461088251506692\n",
            "...1400 tweets downloaded so far\n",
            "getting tweets before 1445819076073328644\n",
            "...1600 tweets downloaded so far\n",
            "getting tweets before 1442996049262493697\n",
            "...1800 tweets downloaded so far\n",
            "getting tweets before 1439633102150635522\n",
            "...2000 tweets downloaded so far\n",
            "getting tweets before 1436130768128917531\n",
            "...2200 tweets downloaded so far\n",
            "getting tweets before 1433763634119729154\n",
            "...2400 tweets downloaded so far\n",
            "getting tweets before 1430462321915088896\n",
            "...2600 tweets downloaded so far\n",
            "getting tweets before 1427079018990374914\n",
            "...2800 tweets downloaded so far\n",
            "getting tweets before 1420841772696031233\n",
            "...3000 tweets downloaded so far\n",
            "getting tweets before 1411116267851845631\n",
            "...3200 tweets downloaded so far\n",
            "getting tweets before 1407423256060063748\n",
            "...3250 tweets downloaded so far\n",
            "getting tweets before 1405977383983669249\n",
            "...3250 tweets downloaded so far\n",
            "                 Tweet_id  ...                                         Tweet_Text\n",
            "0     1478508067213885440  ...  @jack A lot of energy is spent (wasted) debati...\n",
            "1     1478504331057852417  ...                               @griillz_eth I'm in.\n",
            "2     1478502749281001472  ...  @mattmireles @john_c_palmer @makinmarkets Ange...\n",
            "3     1478455370481766400  ...  @jamie247 I'd like to learn more but first nee...\n",
            "4     1478445682075729923  ...  @brianjcho @amytongwu Cosign on those and if y...\n",
            "...                   ...  ...                                                ...\n",
            "3245  1406296934852161549  ...                                   @sriramk Welcome\n",
            "3246  1406240299442065408  ...         @fvckrender @Ioannis_AG @Sothebys Congrats\n",
            "3247  1406226324318945282  ...  @PatrickWStanley Maybe all the BTC accumulatio...\n",
            "3248  1405982338123587588  ...  @JanetWuNews Optional steam release at the cho...\n",
            "3249  1405977383983669250  ...  Washer and dryer units that play quiet techno ...\n",
            "\n",
            "[3250 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# CONNECT TWITTER \n",
        "\n",
        "#! python3\n",
        "\n",
        "#libraries\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import csv\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "#Twitter API credentials\n",
        "consumer_key = 'key'\n",
        "consumer_secret = 'secret'\n",
        "access_key = 'key'\n",
        "access_secret = 'secret'\n",
        "\n",
        "\n",
        "API_KEY = \"api_key\"\n",
        "\n",
        "\n",
        "\n",
        "def validate_twitter(consumer_key, consumer_secret, access_key, access_secret):\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_key, access_secret)\n",
        "\n",
        "  api = tweepy.API(auth)\n",
        "\n",
        "  return api\n",
        "\n",
        "def sort_tweets_by_time_chunk(input_df, column_name='Date', time_chunk=\"week\"):\n",
        "    for x in range(52):\n",
        "      df = input_df[input_df[column_name].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{screen_name}.csv')\n",
        "\n",
        "# get an arbitrary # of tweets\n",
        "def get_tweets(screen_name, num, api):\n",
        "    new_tweets = api.user_timeline(screen_name=screen_name, count=num)\n",
        "    \n",
        "    return new_tweets\n",
        "\n",
        "\n",
        "def get_all_tweets(screen_name, api):\n",
        "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "\n",
        "    #initialize a list to hold all the tweepy Tweets\n",
        "    alltweets = []\n",
        "\n",
        "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "    new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n",
        "\n",
        "    #save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "\n",
        "    #save the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "\n",
        "    #keep grabbing tweets until there are no tweets left to grab\n",
        "    while len(new_tweets) > 0:\n",
        "        print(f\"getting tweets before {oldest}\")\n",
        "\n",
        "        #all subsiquent requests use the max_id param to prevent duplicates\n",
        "        new_tweets = api.user_timeline(screen_name=screen_name,\n",
        "                                       count=200,\n",
        "                                       max_id=oldest)\n",
        "\n",
        "        #save most recent tweets\n",
        "        alltweets.extend(new_tweets)\n",
        "\n",
        "        #update the id of the oldest tweet less one\n",
        "        oldest = alltweets[-1].id - 1\n",
        "\n",
        "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
        "\n",
        "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
        "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text]\n",
        "                 for tweet in alltweets]\n",
        "                 \n",
        "    # Transposed in the repl.it\n",
        "    tweets_df = pd.DataFrame(outtweets,\n",
        "                             columns=[\"Tweet_id\", \"Date\", \"Tweet_Text\"])\n",
        "    \n",
        "    # TODO: incorporate this start to error handling\n",
        "    # except BaseException as e:\n",
        "    #       print('failed on_status', str(e))\n",
        "    #       time.sleep(3)\n",
        "\n",
        "  # For each week in the year, create dir if ! exists, filter df, write to csv, and save to folder\n",
        "  # See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekofyear.html  \n",
        "    '''\n",
        "    for x in range(52):\n",
        "      df = tweets_df[tweets_df['Date'].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{screen_name}.csv')\n",
        "    '''\n",
        "    return tweets_df\n",
        "\n",
        "    \n",
        "# If using datetime object, can use this function to get the week for an individual tweet\n",
        "# Note that every python data type handles this slightly differently! Check the type!\n",
        "def make_chunks(data, tuple_position=1):\n",
        "  # 0 = year, 1 = week, 2 = day\n",
        "  weeks = []\n",
        "  for x in data:\n",
        "    chunk = x.created_at.isocalendar()[tuple_position]\n",
        "    weeks.append(chunk)\n",
        "  return weeks\n",
        "\n",
        "# #tweets\n",
        "# tweets = pd.read_csv('bored_tweets.csv')\n",
        "\n",
        "# #remove links\n",
        "# no_links = tweets[~tweets.text.str.contains(\"http\")]\n",
        "# no_links.head()\n",
        "\n",
        "# # only tweets\n",
        "# just_tweets = no_links.text\n",
        "\n",
        "# # make everything a string\n",
        "# text = str(just_tweets)\n",
        "\n",
        "\n",
        "api = validate_twitter(consumer_key, consumer_secret, access_key, access_secret)\n",
        "test = get_all_tweets('BoredElonMusk', api)\n",
        "print(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "f3UvncJa3w3S",
        "outputId": "99d5704e-01b4-4179-ac53-0c8e179ec6da"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0997c6369d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DISCORD_BOT_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Bot {bot_token}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# CONNECT DISCORD #\n",
        "\n",
        "import requests\n",
        "\n",
        "print(os.environ[\"DISCORD_BOT_TOKEN\"])\n",
        "\n",
        "headers = {\"Authorization\": f\"Bot {bot_token}\"}\n",
        "\n",
        "test_channel_id = 885948250253848588\n",
        "test_guild_id = 647140436686667776\n",
        "\n",
        "channel_path = f'https://discord.com/api/channels/{test_channel_id}/messages'\n",
        "\n",
        "guild_path_channels = f'https://discord.com/api/guilds/{test_guild_id}/channels'\n",
        "\n",
        "\n",
        "res = requests.get(channel_path, headers=headers).json()\n",
        "\n",
        "for x in res:\n",
        "  print(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et_1gAPgUWLI"
      },
      "outputs": [],
      "source": [
        "# SUPPORT FUNCTIONS #\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Lazy util function\n",
        "def check_type(data):\n",
        "  print(type(data))\n",
        "\n",
        "def NUKE_ALL_FOLDERS_AND_FILES():\n",
        "  print(\"Nuclear launch detected\")\n",
        "  # Get directory name\n",
        "  # TODO: Update this to be more flexible, but it works for now\n",
        "  for x in range(1,52):\n",
        "    try:\n",
        "        shutil.rmtree(f'week-{x}')\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "\n",
        "#NUKE_ALL_FOLDERS_AND_FILES()\n",
        "\n",
        "#os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LllFBPZDdN5u",
        "outputId": "bb405c9c-cb1c-4f7a-fb93-f00628197102"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# TODO: Objectify functions\n",
        "\n",
        "def generate_df(input_df, file_path_name):\n",
        "  '''\n",
        "  - For x in df, grab x['date'] and split into day/month/year/week of year\n",
        "  - Make columns for each val and assign accordingly\n",
        "  - This will result in one large spreadsheet that you can filter\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "def parse_by_weeks(input_df, file_path_name):\n",
        "    for x in range(52):\n",
        "      df = input_df[input_df['Date'].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{file_path_name}.csv')\n",
        "    return input_df\n",
        "\n",
        "  \n",
        "generate_data(test, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziGgbGGxLt76"
      },
      "outputs": [],
      "source": [
        "### DRAFT Collection Object\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "class DataCollection():\n",
        "  def __init__(self, token, output_columns=[\"id\", \"post_date\", \"author\", \"content\"]):\n",
        "        self.token = token\n",
        "        self.output_columns = output_columns\n",
        "  def make_df(self, input_arr):\n",
        "    df = pd.DataFrame(self, input_arr, columns=self.output_columns)\n",
        "    return df\n",
        "  def help(self, service=None):\n",
        "    message = \"This will be a spot to outline the various functions and required bits for each service. Pass arg 'service=' for help with specific services\"\n",
        "    # Match is not yet implemented in Colab. Sad day.\n",
        "    '''\n",
        "    match service:\n",
        "            case ‘discord’ : message += \"Discord requires a BOT token\"\n",
        "    '''\n",
        "    if service == 'discord':\n",
        "      message += '\\n Discord requires a BOT token'\n",
        "    return message\n",
        "\n",
        "# TOOD: redo pagination\n",
        "class TwitterCollection(DataCollection):\n",
        "  # NOTES: Twitter's requests are formatted such that you need to look up the user id first.\n",
        "  def get_user(self, screen_name):\n",
        "    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
        "    req = requests.get(f'https://api.twitter.com/2/users/by/username/{screen_name}', headers=headers)\n",
        "    return req\n",
        "    \n",
        "  def get_tweets(self, screen_name, num):\n",
        "    user_id = self.get_user(screen_name).json()['data']['id']\n",
        "    #tweets_list = (screen_name=screen_name, count=num)\n",
        "    #req = requests.get( )\n",
        "    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
        "    req = requests.get(f'https://api.twitter.com/2/users/{user_id}/tweets', headers=headers) \n",
        "    return req\n",
        "\n",
        "\n",
        "class DiscordCollection(DataCollection):\n",
        "  def get_channel_messages(self, channel, before=None, after=None, around=None, count=None):\n",
        "    headers = {\"Authorization\": f\"Bot {self.token}\"}\n",
        "    count = count if count else 100\n",
        "    before = f'&before={before}' if before else \"\"\n",
        "    after = f'&after={after}' if after else \"\"\n",
        "    around = f'&around={around}' if around else \"\"\n",
        "    channel_path = f'https://discord.com/api/channels/{channel}/messages?&limit={count}'\n",
        "    allmessages = []\n",
        "    try:\n",
        "        new_messages = requests.get(channel_path, headers=headers).json() \n",
        "        allmessages.extend(new_messages)\n",
        "        print(len(allmessages))\n",
        "        #save the id of the oldest tweet less one\n",
        "        oldest = int(allmessages[-1]['id']) - 1\n",
        "        print(int(len(allmessages)) < count)\n",
        "        #keep grabbing messages until there are no messages left to grab\n",
        "        if int(len(allmessages)) < count:\n",
        "          while len(new_messages) > 0:\n",
        "              print(len(new_messages))\n",
        "              print(f\"getting messages before {oldest}\")\n",
        "              channel_path = f'https://discord.com/api/channels/{channel}/messages?limit={count}&before={oldest}'\n",
        "              new_messages = requests.get(channel_path, headers=headers).json() \n",
        "\n",
        "              allmessages.extend(new_messages)\n",
        "\n",
        "              print(f\"...{len(allmessages)} messages downloaded so far\")\n",
        "\n",
        "              try:\n",
        "                  oldest = int(allmessages[-1]['id']) - 1\n",
        "              except:\n",
        "                  print(\"Done\")\n",
        "                  break\n",
        "          print(len(allmessages))\n",
        "        return allmessages\n",
        "    except Exception as e: print(f'Error: {e}')\n",
        "\n",
        "  def get_user_messages(self, user, before=None, after=None, around=None, count=None):\n",
        "    # Should return messages from a specific user\n",
        "    pass\n",
        "\n",
        "  def make_arr(self, messages):\n",
        "      message_arr = [{m['id'], m['timestamp'], m['author']['username'], m['content']} for m in messages]\n",
        "      return message_arr\n",
        "\n",
        "\n",
        "class KaggleCollection(DataCollection):\n",
        "  pass\n",
        "\n",
        "\n",
        "# Reddit requires a temporary oAuth token.\n",
        "# NOTE THAT YOU MUST UPDATE YOUR USER AGENT STRING WITH YOUR OWN INFO IF NOT USING MY CREDS OR YOUR APP WILL BE BANNED\n",
        "def get_reddit_token(client, secret, username, password, user_agent_string='data_test:0.0.1 ((by /u/person_of_note)'):\n",
        "  # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
        "    auth = requests.auth.HTTPBasicAuth(client, secret)\n",
        "\n",
        "    # here we pass our login method (password), username, and password\n",
        "    data = {'grant_type': 'password',\n",
        "            'username': username,\n",
        "            'password': password}\n",
        "\n",
        "    # setup our header info, which gives reddit a brief description of our app\n",
        "    headers = {'User-Agent': user_agent_string}\n",
        "\n",
        "    # send our request for an OAuth token\n",
        "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
        "                        auth=auth, data=data, headers=headers)\n",
        "\n",
        "    # convert response to JSON and pull access_token value\n",
        "    oauth_token = res.json()['access_token']\n",
        "    # add authorization to our headers dictionary\n",
        "    return oauth_token\n",
        "\n",
        "# TODO: check for oauth expiry and get new token automatically\n",
        "class RedditCollection(DataCollection):\n",
        "  def get_subreddit(self, subreddit, filter=\"Hot\"):\n",
        "    headers = {'Authorization': f\"bearer {self.token}\"}\n",
        "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
        "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/{filter}\",\n",
        "                      headers=headers)\n",
        "    print(res.status_code)\n",
        "    if res.status_code == 200:\n",
        "      for post in res.json()['data']['children']:\n",
        "          print(post['data']['title'])\n",
        "    elif res.status_code > 400:\n",
        "      print('Too many requests, please try again later')\n",
        "    else:\n",
        "      print(\"Invalid token, please get a new token using the get_token function\")\n",
        "\n",
        "class SlackCollection(DataCollection):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "BjJyFpemeKn2",
        "outputId": "500f3009-133b-480b-8c36-2d16b37dda58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "False\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-47ccc3921386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdiscord_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestDiscord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscord_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdiscord_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestDiscord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscord_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscord_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m            \u001b[0mTo\u001b[0m \u001b[0mpreserve\u001b[0m \u001b[0mdtypes\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0miterating\u001b[0m \u001b[0mover\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m            \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mnamedtuples\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m            \u001b[0;32mand\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgenerally\u001b[0m \u001b[0mfaster\u001b[0m \u001b[0mthan\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_string\u001b[0;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, min_rows, max_cols, show_dimensions, decimal, line_width, max_colwidth, encoding)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mcompute\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mmultiplication\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIn\u001b[0m \u001b[0maddition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0mDataFrame\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mother\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0maligned\u001b[0m \u001b[0mprior\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmultiplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mdot\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mSeries\u001b[0m \u001b[0mcomputes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minner\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_string\u001b[0;34m(self, buf, encoding, line_width)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_formatted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;31m# Note: this is only used by to_string() and to_latex(), not by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m         \u001b[0;31m# to_html(). so safe to cast col_space here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mcol_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.string'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "testDiscord = DiscordCollection(os.environ[\"DISCORD_BOT_TOKEN\"])\n",
        "discord_messages = testDiscord.get_channel_messages(885948250253848588, count=2)\n",
        "discord_arr = testDiscord.make_arr(discord_messages)\n",
        "discord_df = testDiscord.make_df(discord_arr)\n",
        "print(discord_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdOwovr6XHQk"
      },
      "source": [
        "OBJECT DESIGN NOTES:\n",
        "\n",
        "Possibilities:\n",
        "\n",
        "* Object takes in a df that has been formatted elsewhere. Object would focus primarily on methods that manipulate the data into different file formats and sorting methods.\n",
        "    * Relies on the user to clean the data and make dataframes that conform to a predtermined column format, which is less magical than the next design\n",
        "    * Follows DRY, SRP, KISS and YAGNI\n",
        "    * Pseudo-code examples:\n",
        "        * `weeks_dataset = new DataSet.weeks(df, filter_by=\"weeks\")`\n",
        "        * `weeks_and_years_dataset = new DataSet.weeks(df, filter_by=['weeks','years'])`\n",
        "\n",
        "* Include methods for scraping the most common datasets, something like:\n",
        "    * Object would attempt to parse out a date object by searching for a column/split point marked as \"date,\" \"created_at\" or similar\n",
        "    * Relies less on end user\n",
        "    * Runs the risk of getting really bloated and trying too hard to cover every edge case for every random data source\n",
        "    * pseudo-code examples:\n",
        "        * `kaggle_data = new DataSet.kaggle(credentials, dataset_name)` (are Kaggle datasets standardized, or are there a ton of different presentations? I think the latter)\n",
        "        * `discord_data = new DataSet.discord(credentials, args* kwargs*)`\n",
        "        * `twittter_data = new DataSet.twitter(< adapt existing function from this notebook>)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvyGEWTBHGX5"
      },
      "source": [
        "# TYPE ONE:\n",
        "# This is not exact; it is late at night and mostly an example, I don't remember the exact syntax off the top of my head\n",
        "class CleanDataSet:\n",
        "    def __init__(self, df, file_prefix):\n",
        "        self.input_df = input_df\n",
        "        self.file_prefix = file_prefix\n",
        "        \n",
        "    def parse_by_weeks(self):\n",
        "      for x in range(52):\n",
        "        df = input_df[input_df['Date'].dt.isocalendar().week == x]\n",
        "        print(df.count)\n",
        "        # Only make folders and files for weeks where there were tweets\n",
        "        if not df.empty:\n",
        "          if not os.path.exists(f'week-{x}'):\n",
        "            os.makedirs(f'week-{x}')\n",
        "          df.to_csv(f'week-{x}/{file_prefix}.csv')\n",
        "      return input_df\n",
        "\n",
        "\n",
        "\n",
        "# TYPE TWO:\n",
        "\n",
        "class ScrapeDataSet:\n",
        "    def __init__(self, data_source, file_prefix):\n",
        "        self.data_source = data_source\n",
        "        self.file_prefix = file_prefix\n",
        "        \n",
        "    def kaggle(credentials, dataset):\n",
        "      # Fetch dataset by path (see cell )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kXDVzJq37QI"
      },
      "outputs": [],
      "source": [
        "#HELPER FUNCTIONS FOR ENRON DATA PARSING\n",
        "\n",
        "import email\n",
        "\n",
        "email_df = pd.read_csv('enron_path') #just change this to the right path and run these two cells\n",
        "\n",
        "def get_text_from_email(msg):\n",
        "    '''To get the content from email objects'''\n",
        "    parts = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            parts.append( part.get_payload() )\n",
        "    return ''.join(parts)\n",
        "\n",
        "def split_email_addresses(line):\n",
        "    '''To separate multiple email addresses'''\n",
        "    if line:\n",
        "        addrs = line.split(',')\n",
        "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
        "    else:\n",
        "        addrs = None\n",
        "    return addrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nJzypbR4eDw"
      },
      "outputs": [],
      "source": [
        "# Parse the emails into a list email objects\n",
        "messages = list(map(email.message_from_string, emails_df['message']))\n",
        "emails_df.drop('message', axis=1, inplace=True)\n",
        "# Get fields from parsed email objects\n",
        "keys = messages[0].keys()\n",
        "for key in keys:\n",
        "    emails_df[key] = [doc[key] for doc in messages]\n",
        "# Parse content from emails\n",
        "emails_df['content'] = list(map(get_text_from_email, messages))\n",
        "# Split multiple email addresses\n",
        "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
        "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
        "\n",
        "# Extract the root of 'file' as 'user'\n",
        "emails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\n",
        "del messages\n",
        "\n",
        "emails_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqw8pdXL1xmm"
      },
      "outputs": [],
      "source": [
        "#get topics using empath\n",
        "\n",
        "#!pip install empath\n",
        "\n",
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "#sentence to analyze\n",
        "content = emails_df['content'][2]\n",
        "\n",
        "#analyze\n",
        "nlp = lexicon.analyze(content, normalize=True)\n",
        "\n",
        "#print the content\n",
        "print(content)\n",
        "#print topics with scores greater than 0.5 \n",
        "for key, v in nlp.items():\n",
        "    if v > 0.05:\n",
        "        print(key, v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX65Kc7C10ik"
      },
      "outputs": [],
      "source": [
        "#join empath results with emails_df where empath results are greater than 0.5\n",
        "#subset emails df to make this run faster for testing\n",
        "emails_df_subset = emails_df[:100]\n",
        "\n",
        "emails_df_subset['empath'] = \"\"\n",
        "for i, row in emails_df_subset.iterrows():\n",
        "    content = row['content']\n",
        "    nlp = lexicon.analyze(content, normalize=True)\n",
        "    for key, v in nlp.items():\n",
        "        if v > 0.05:\n",
        "            emails_df_subset.at[i, 'empath'] = key\n",
        "\n",
        "emails_df_subset['empath'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwG2Hvl13wo"
      },
      "outputs": [],
      "source": [
        "#train gpt3 classifier on emapth topcis\n",
        "import openai\n",
        "import config\n",
        "#for gpt2 if needed\n",
        "#import gpt_2_simple as gpt2\n",
        "\n",
        "\n",
        "\n",
        "openai.api_key = config.openai_key\n",
        "\n",
        "#create new df for training\n",
        "train_df = emails_df_subset[['content', 'empath']]\n",
        "#make sure no null values\n",
        "train_df = train_df[train_df['empath'] != \"\"]\n",
        "#im getting errors about the length of the content examples so I need to only select ones that are less than 500 characters till i figure out how to fix this\n",
        "train_df = train_df[train_df['content'].str.len() < 30]\n",
        "\n",
        "#create a dictionary of examples\n",
        "examples_dict = dict(zip(train_df.content, train_df.empath))\n",
        "#for the sake of this test we will reduce the dict down to the first 3 items or gpt3 will error -- will need to train on a csv for more\n",
        "examples_dict = {k: examples_dict[k] for k in list(examples_dict)[:3]}\n",
        "#print(examples_dict)\n",
        "\n",
        "#turn examples into a list of lists for gpt3 formatting\n",
        "examples = list(map(list, examples_dict.items()))\n",
        "\n",
        "#get unique values from the list for the training labels\n",
        "labels = list(set([x for x in examples_dict.values()]))\n",
        "#print(labels)\n",
        "\n",
        "#select random content from emails_df to classify\n",
        "from random import randrange\n",
        "num = randrange(200000)\n",
        "\n",
        "query = emails_df['content'][num]\n",
        "\n",
        "#remove URLS from query\n",
        "import re\n",
        "query = re.sub(r'http\\S+', '', query)\n",
        "\n",
        "#print(query)\n",
        "\n",
        "def gpt_classify(examples, labels, query):\n",
        "  response = openai.Classification.create(\n",
        "        search_model=\"ada\",\n",
        "        model=\"curie\",\n",
        "        examples=[examples],\n",
        "        query=query,\n",
        "        labels=labels,\n",
        "      )\n",
        "  return response.label\n",
        "\n",
        "test = gpt_classify(examples, labels, query)\n",
        "\n",
        "print(test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
